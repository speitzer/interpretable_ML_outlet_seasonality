{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65ff950",
   "metadata": {},
   "source": [
    "# Getting Started with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35f816",
   "metadata": {},
   "source": [
    "After interpolation, SSA, and 'create_main_df_per_glacier', data should be ready for the XGBoost model (or similar machine learning model) coded below. If you're pre-processing your own data, all input variables should have data for the same dates in order to use in this model.\n",
    "\n",
    "This code includes many plot outputs that you can comment off if you're not interested in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import os\n",
    "import json\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipynbname\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy import stats\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "import copy\n",
    "from PIL import Image, ImageChops\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", message=\".*The 'nopython' keyword.*\")  # from shap\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b54538-fa5f-4e71-9429-cbae7edd9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set font style for consistency\n",
    "mpl.rcParams['font.family'] = 'Arial'  # Or any other preferred font\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"axes.titlesize\": 6,\n",
    "    \"axes.labelsize\": 6,\n",
    "    \"xtick.labelsize\": 6,\n",
    "    \"ytick.labelsize\": 6,\n",
    "    \"legend.title_fontsize\": 6,\n",
    "    \"legend.fontsize\": 6\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c8956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "\n",
    "# objective function for hyperparameters\n",
    "def objective(params):\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = -r2_score(y_test, y_pred)  # Negative R2 score for minimization\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def compute_offsets(data, predicted_col):\n",
    "    offsets = []\n",
    "    abs_offsets = []  # List to store absolute offsets\n",
    "    \n",
    "    # Ensure dates are datetime objects\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    \n",
    "    # Find annual max and min from observed data\n",
    "    for year in data['Date'].dt.year.unique():\n",
    "        year_data = data[data['Date'].dt.year == year]\n",
    "        if year_data.empty:\n",
    "            continue\n",
    "            \n",
    "        # Maxima\n",
    "        data_max_idx = argrelextrema(year_data['Seasonal Terminus Position'].values, np.greater, order=2)[0]\n",
    "        pred_max_idx = argrelextrema(year_data[predicted_col].values, np.greater, order=2)[0]\n",
    "        offsets += calculate_nearest_offsets(year_data, data_max_idx, pred_max_idx)\n",
    "        abs_offsets += [abs(offset) for offset in calculate_nearest_offsets(year_data, data_max_idx, pred_max_idx)]\n",
    "        \n",
    "        # Minima\n",
    "        data_min_idx = argrelextrema(year_data['Seasonal Terminus Position'].values, np.less, order=2)[0]\n",
    "        pred_min_idx = argrelextrema(year_data[predicted_col].values, np.less, order=2)[0]\n",
    "        offsets += calculate_nearest_offsets(year_data, data_min_idx, pred_min_idx)\n",
    "        abs_offsets += [abs(float(offset)) for offset in calculate_nearest_offsets(year_data, data_min_idx, pred_min_idx)]        \n",
    "             \n",
    "    mean_offset = np.mean(offsets) if offsets else np.nan\n",
    "    mean_abs_offset = np.mean(abs_offsets) if abs_offsets else np.nan  # Mean of absolute offsets\n",
    "    \n",
    "    return mean_offset, mean_abs_offset\n",
    "\n",
    "\n",
    "def calculate_nearest_offsets(data, data_extrema_idx, pred_extrema_idx):\n",
    "    offsets = []\n",
    "    if len(data_extrema_idx) == 0 or len(pred_extrema_idx) == 0:\n",
    "        return offsets\n",
    "    \n",
    "    for data_idx in data_extrema_idx:\n",
    "        data_date = data.iloc[data_idx]['Date']\n",
    "        nearest_pred_idx = np.argmin(np.abs(data['Date'].iloc[pred_extrema_idx] - data_date).dt.days)\n",
    "        pred_date = data.iloc[pred_extrema_idx[nearest_pred_idx]]['Date']\n",
    "        offset = (pred_date - data_date).days\n",
    "        offsets.append(offset)\n",
    "        \n",
    "    return offsets\n",
    "\n",
    "\n",
    "def trim_image_whitespace(image_path, output_path=None, bgcolor=(255, 255, 255), fuzz=10):\n",
    "    \"\"\"\n",
    "    Trims the border of an image by detecting background color (usually white).\n",
    "    `fuzz` controls how tolerant the trim is to color variations.\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Create a background image of the same size with the background color\n",
    "    bg = Image.new(img.mode, img.size, bgcolor)\n",
    "\n",
    "    # Find the difference\n",
    "    diff = ImageChops.difference(img, bg)\n",
    "\n",
    "    # Increase sensitivity by converting to grayscale and amplifying\n",
    "    diff = diff.convert(\"L\")\n",
    "    diff = diff.point(lambda x: 0 if x <= fuzz else 255, mode='1')\n",
    "\n",
    "    bbox = diff.getbbox()\n",
    "    if bbox:\n",
    "        cropped_img = img.crop(bbox)\n",
    "        save_path = output_path or image_path\n",
    "        cropped_img.save(save_path)\n",
    "        return save_path\n",
    "    else:\n",
    "        return image_path  # no trimming needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296deb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open glacierid file\n",
    "file_path = 'processing_scripts/qualifying_glacierids.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    glacier_ids = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f80b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main iteration\n",
    "for glacierid in glacier_ids:\n",
    "    glacierid = glacierid\n",
    "    print('Working on glacier: '+glacierid)\n",
    "    filepath = \".../all_features_per_glacier/\"\n",
    "    file_path = os.path.join(filepath, f\"{glacierid}_main_df.csv\")\n",
    "    data = pd.read_csv(file_path, header=0)\n",
    "    y = data['Seasonal Terminus Position']  # target variable\n",
    "    # drop any unused variables from your df\n",
    "    X = data.drop(columns=['Date', 'Seasonal Terminus Position', 'Seasonal Discharge-RACMO', 'Seasonal OTF-ECCO'])\n",
    "    current_column_names = X.columns\n",
    "    # rename variable names if desired\n",
    "    new_column_names = ['Air Temperature','Runoff','Velocity',\n",
    "                       'Strain Rate','Surface Elevation','Bed Elevation',\n",
    "                       'Thickness','Slope','Melange','Ocean Thermal Forcing']\n",
    "    column_rename_dict = dict(zip(current_column_names, new_column_names))\n",
    "    X.rename(columns=column_rename_dict, inplace=True)\n",
    "    # set train/test data split; test_size=0.2 for 80/20 split.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    # Define the search space for hyperparameters\n",
    "    param_space = {\n",
    "        'tree_method': hp.choice('tree_method', ['approx']),  \n",
    "        'objective': hp.choice('objective', ['reg:squarederror']),  \n",
    "        'n_estimators': scope.int(hp.quniform('n_estimators', 50, 300, 10)),\n",
    "        'max_depth': scope.int(hp.randint('max_depth', 2, 10)),  # Convert to int explicitly\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "        'alpha': hp.uniform('alpha', 0, 100), \n",
    "        'reg_lambda': hp.uniform('reg_lambda', 1, 20), \n",
    "        'max_delta_step': scope.int(hp.randint('max_delta_step', 0, 50)),  # Convert to int explicitly\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.2, 1.0) \n",
    "    }\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective,\n",
    "                space=param_space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=50,\n",
    "                trials=trials,\n",
    "                rstate=np.random.default_rng(42))\n",
    "\n",
    "    best_params = {\n",
    "        'tree_method': 'approx',\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': int(best['n_estimators']),\n",
    "        'max_depth': int(best['max_depth']),\n",
    "        'learning_rate': best['learning_rate'],\n",
    "        'alpha': best['alpha'],\n",
    "        'reg_lambda': best['reg_lambda'],\n",
    "        'max_delta_step': best['max_delta_step'],\n",
    "        'colsample_bytree': best['colsample_bytree']\n",
    "    }\n",
    "    \n",
    "    # Define evaluation dataset\n",
    "    evalset = [(X_train, y_train), (X_test, y_test)]\n",
    "    final_model = xgb.XGBRegressor(**best_params,random_state=42)\n",
    "    final_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=evalset,\n",
    "        verbose=False\n",
    "    )\n",
    "    pred_train = final_model.predict(X_train)\n",
    "    pred_test = final_model.predict(X_test)\n",
    "    train_r2 = r2_score(y_train, pred_train)\n",
    "    test_r2 = r2_score(y_test, pred_test)\n",
    "    results = final_model.evals_result()\n",
    "\n",
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results['validation_0']['rmse'], label='Train')\n",
    "    plt.plot(results['validation_1']['rmse'], label='Test')\n",
    "    plt.legend()\n",
    "    plt.title('Root Mean Squared Error')\n",
    "    output_dir = os.path.join(os.getcwd(), 'outputs', 'RMSE_plot')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_path = os.path.join(output_dir, f\"{glacierid}_rmse_plot.png\")\n",
    "    plt.savefig(file_path)\n",
    "    plt.clf()  \n",
    "    plt.cla() \n",
    "    plt.close()\n",
    "    \n",
    "    # Predict Seasonal Comp\n",
    "    data['Predicted Seasonal Comp'] = final_model.predict(X)\n",
    "    data3 = pd.DataFrame()\n",
    "    data3['Predicted Seasonal Comp'] = final_model.predict(X_train)\n",
    "    train_rmse = mean_absolute_error(y_train.values, data3['Predicted Seasonal Comp'].values)\n",
    "    data2 = pd.DataFrame()\n",
    "    data2['Predicted Seasonal Comp'] = final_model.predict(X_test)\n",
    "    test_rmse = mean_absolute_error(y_test.values, data2['Predicted Seasonal Comp'].values)\n",
    "    \n",
    "    # Normalized RMSE using average max minus average min of testing data\n",
    "    avg_max = np.mean([np.max(y_test.values[i::1]) for i in range(1)])  # Assuming annual cycles\n",
    "    avg_min = np.mean([np.min(y_test.values[i::1]) for i in range(1)])\n",
    "    normalization_factor = avg_max - avg_min\n",
    "\n",
    "    train_nrmse = train_rmse / normalization_factor\n",
    "    test_nrmse = test_rmse / normalization_factor\n",
    "    \n",
    "    # add the Spearman correlation coefficient\n",
    "    res = stats.spearmanr(y_test, pred_test)\n",
    "    spearman_stat = res.statistic\n",
    "    spearman_p    = res.pvalue\n",
    "    \n",
    "    # add R2 computation\n",
    "    test_r2 = r2_score(y_test, pred_test)\n",
    "    \n",
    "    new_entry = pd.DataFrame({\n",
    "        'glacierid'         : [glacierid],\n",
    "        'Test RMSE'         : [test_rmse],\n",
    "        'Test NRMSE'        : [test_nrmse],\n",
    "        'Spearman Statistic': [spearman_stat],\n",
    "        'Spearman P-Value'  : [spearman_p],\n",
    "        'Test R2 Score'     : [test_r2]\n",
    "    })\n",
    "    \n",
    "    \n",
    "    # save the output\n",
    "    output_file = os.path.join(os.getcwd(), 'outputs', 'error_scores_nrmse.csv')\n",
    "    if os.path.exists(output_file):\n",
    "        existing_data = pd.read_csv(output_file)\n",
    "        updated_data = pd.concat([existing_data, new_entry], ignore_index=True)\n",
    "        updated_data['glacierid'] = updated_data['glacierid'].astype(int).astype(str).str.zfill(3)\n",
    "        updated_data = updated_data.drop_duplicates(subset=['glacierid'])\n",
    "        updated_data = updated_data.sort_values(by='glacierid')\n",
    "    else:\n",
    "        updated_data = new_entry\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    updated_data.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Compute offsets and absolute offsets\n",
    "    mean_offset, mean_abs_offset = compute_offsets(data, 'Predicted Seasonal Comp')\n",
    "    offset_entry = pd.DataFrame({\n",
    "        'glacierid': [glacierid],\n",
    "        'mean offset': [float(mean_offset)],  \n",
    "        'absolute offset': [float(mean_abs_offset)] \n",
    "    })\n",
    "\n",
    "    offset_file = os.path.join(os.getcwd(), 'outputs', 'error_scores_offset.csv')\n",
    "    if os.path.exists(offset_file):\n",
    "        existing_offset = pd.read_csv(offset_file)\n",
    "        updated_offset = pd.concat([existing_offset, offset_entry], ignore_index=True)\n",
    "        updated_offset['glacierid'] = updated_offset['glacierid'].astype(int).astype(str).str.zfill(3)\n",
    "        updated_offset = updated_offset.drop_duplicates(subset=['glacierid'], keep='last')\n",
    "    else:\n",
    "        updated_offset = offset_entry\n",
    "\n",
    "    os.makedirs(os.path.dirname(offset_file), exist_ok=True)\n",
    "    updated_offset.to_csv(offset_file, index=False)\n",
    "\n",
    "    # Save prediction plot\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    predicted_data = data[['Date', 'Predicted Seasonal Comp']].copy()\n",
    "    test_indices = X_test.index\n",
    "    test_period_start = data.loc[test_indices, 'Date'].min()\n",
    "    test_period_end = data.loc[test_indices, 'Date'].max()\n",
    "    predicted_data_test = predicted_data[(predicted_data['Date'] >= test_period_start) & \n",
    "                                         (predicted_data['Date'] <= test_period_end)]\n",
    "    plt.figure(figsize=(1.9, 1.9))\n",
    "    plt.subplots_adjust(left=0.13, right=0.95)  \n",
    "    plt.plot(data['Date'], data['Seasonal Terminus Position'], color='green', linewidth=1, label='Actual')\n",
    "    plt.plot(predicted_data_test['Date'], predicted_data_test['Predicted Seasonal Comp'],\n",
    "             color='black', alpha=0.6, linewidth=1, label='Model')\n",
    "    plt.axvspan(test_period_start, test_period_end, facecolor='green', alpha=0.1)\n",
    "    plt.ylabel('Terminus Advance (m)', labelpad=2.5)  \n",
    "    ax = plt.gca()\n",
    "    ax.tick_params(axis='y', pad=2)  \n",
    "    plt.legend(loc='upper left', ncol=2, markerscale=0.7, handletextpad=0.2, borderpad=0.10, labelspacing=2)\n",
    "\n",
    "    output_dir = os.path.join(os.getcwd(), 'outputs', 'prediction_plot')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, f'{glacierid}_prediction.plot.png')\n",
    "    plt.savefig(output_file, dpi=300,\n",
    "                format='png', metadata={'Creator': 'Matplotlib'},\n",
    "               bbox_inches='tight')\n",
    "    plt.clf()  \n",
    "    plt.cla()  \n",
    "    plt.close()\n",
    "    trim_image_whitespace(output_file, bgcolor=(255, 255, 255), fuzz=10)\n",
    "\n",
    "    \n",
    "    # Create an explainer for the model using SHAP\n",
    "    explainer = shap.Explainer(final_model.predict, X)\n",
    "    shap_test = explainer(X_test)\n",
    "    \n",
    "    # Define the new feature names for figures\n",
    "    abs_feature_names = [\n",
    "        'Air Temp.', 'Runoff', 'Velocity', 'Strain Rate', \n",
    "        'Surface Ele.', 'Bed Ele.', 'Thickness', \n",
    "        'Bed Slope', 'Mélange', 'OTF'\n",
    "    ]\n",
    "    rel_feature_names = [\n",
    "        'Air Temp. (°C)', 'Runoff (m³/s)', 'Velocity (m/yr)', 'Strain R. (yr$^{-1}$)', \n",
    "        'Surface Ele. (m)', 'Bed Ele. (m)', 'Thickness (m)', \n",
    "        'Bed Slope (m)', 'Mélange (°C)', 'OTF (°C)'\n",
    "    ]\n",
    "\n",
    "    abs_shap_test = copy.deepcopy(shap_test)\n",
    "    rel_shap_test = copy.deepcopy(shap_test)\n",
    "    abs_shap_test.feature_names = abs_feature_names\n",
    "    rel_shap_test.feature_names = rel_feature_names\n",
    "    \n",
    "    # SHAP values - absolute names\n",
    "    mean_abs_shap_values = np.abs(abs_shap_test.values).mean(axis=0)\n",
    "    sorted_feature_indices = np.argsort(mean_abs_shap_values)[::-1]\n",
    "    sorted_feature_names = [abs_shap_test.feature_names[i] for i in sorted_feature_indices]\n",
    "    shap_df = pd.DataFrame(abs_shap_test.values, columns=abs_shap_test.feature_names, index=X_test.index)\n",
    "    shap_df = shap_df[sorted_feature_names]\n",
    "    test_period_start = data.loc[X_test.index.min(), 'Date']\n",
    "    test_period_end = data.loc[X_test.index.max(), 'Date']\n",
    "    test_dates = data[(data['Date'] >= test_period_start) & (data['Date'] <= test_period_end)]['Date']\n",
    "    shap_df.index = test_dates.values\n",
    "    shap_values_dir = \"/Users/kevin/Documents/ML_longterm/XGBoost/seasonal/full_features/outputs/shap_values\"\n",
    "    os.makedirs(shap_values_dir, exist_ok=True)\n",
    "    shap_values_filename = os.path.join(shap_values_dir, f\"{glacierid}_shap_values.csv\")\n",
    "    shap_df.to_csv(shap_values_filename)\n",
    "\n",
    "    # SHAP values - relationship names\n",
    "    rel_mean_abs_shap_values = np.abs(rel_shap_test.values).mean(axis=0)\n",
    "    rel_sorted_feature_indices = np.argsort(rel_mean_abs_shap_values)[::-1]\n",
    "    rel_sorted_feature_names = [rel_shap_test.feature_names[i] for i in rel_sorted_feature_indices]\n",
    "    rel_shap_df = pd.DataFrame(rel_shap_test.values, columns=rel_shap_test.feature_names, index=X_test.index)\n",
    "    rel_shap_df = rel_shap_df[rel_sorted_feature_names]\n",
    "    test_period_start = data.loc[X_test.index.min(), 'Date']\n",
    "    test_period_end = data.loc[X_test.index.max(), 'Date']\n",
    "    test_dates = data[(data['Date'] >= test_period_start) & (data['Date'] <= test_period_end)]['Date']\n",
    "    rel_shap_df.index = test_dates.values\n",
    "\n",
    "    # SHAP Percentages\n",
    "    mean_absolute_values = shap_df.abs().mean()\n",
    "    total_sum = mean_absolute_values.sum()\n",
    "    ranking_df = (mean_absolute_values / total_sum * 100).round(2)\n",
    "    ranking_df = ranking_df.to_frame().T\n",
    "    ranking_df.insert(0, 'glacierid', glacierid)\n",
    "    shap_percentages_file = \"/Users/.../outputs/shap_percentages.csv\"\n",
    "    if os.path.exists(shap_percentages_file):\n",
    "        existing_df = pd.read_csv(shap_percentages_file)\n",
    "        existing_df = pd.concat([existing_df, ranking_df], ignore_index=True)\n",
    "        existing_df['glacierid'] = existing_df['glacierid'].astype(int).astype(str).str.zfill(3)\n",
    "        existing_df = existing_df.drop_duplicates(subset=['glacierid'], keep='last')\n",
    "        existing_df = existing_df.sort_values(by='glacierid')\n",
    "    else:\n",
    "        existing_df = ranking_df\n",
    "    existing_df.to_csv(shap_percentages_file, index=False)\n",
    "    \n",
    "    # SHAP Mean Plot\n",
    "    mean_shap_dir = \"/Users/.../outputs/shap_mean_plots\"\n",
    "    summary_shap_dir = \"/.../outputs/shap_summary_plots\"\n",
    "    os.makedirs(mean_shap_dir, exist_ok=True)\n",
    "    os.makedirs(summary_shap_dir, exist_ok=True)\n",
    "    columns = shap_df.abs().mean().sort_values(ascending=False).index\n",
    "    feature_categories = {\n",
    "        'Bed Ele.': ('Geometric', 'tab:blue'),\n",
    "        'Surface Ele.': ('Geometric', 'tab:blue'),\n",
    "        'Bed Slope': ('Geometric', 'tab:blue'),\n",
    "        'Thickness': ('Geometric', 'tab:blue'),\n",
    "        'Velocity': ('Dynamic', '#FFD700'),\n",
    "        'Strain Rate': ('Dynamic', '#FFD700'),\n",
    "        'OTF': ('Climatic', 'tab:red'),\n",
    "        'Air Temp.': ('Climatic', 'tab:red'),\n",
    "        'Runoff': ('Climatic', 'tab:red'),\n",
    "        'Mélange': ('Climatic', 'tab:red'),\n",
    "    }\n",
    "    legend_dict = {}\n",
    "    legend_colors = []\n",
    "    labels = []\n",
    "    for column in columns:\n",
    "        category, color = feature_categories.get(column, ('Unknown', 'gray'))\n",
    "        if category not in legend_dict:\n",
    "            legend_dict[category] = color\n",
    "        labels.append(category)\n",
    "        legend_colors.append(color)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(1.1, 1.8))\n",
    "    plt.subplots_adjust(left=0.05)  \n",
    "    sns.barplot(x=shap_df[columns].abs().mean(), y=columns, palette=legend_colors, ax=ax)\n",
    "    ax.set_title(\"\")  # Remove the title\n",
    "    ax.set_xlabel(\"Absolute SHAP Value\")  \n",
    "    ax.set_ylabel(\"\") \n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=6) for color in legend_dict.values()]\n",
    "    ax.legend(handles, legend_dict.keys(), \n",
    "        loc='lower right', \n",
    "        handletextpad=0.3, borderpad=0.15,labelspacing=0.2\n",
    "    )\n",
    "    mean_shap_filepath = os.path.join(mean_shap_dir, f\"{glacierid}_mean_shap.png\")\n",
    "    plt.savefig(mean_shap_filepath, dpi=300, bbox_inches='tight')\n",
    "    plt.clf()  \n",
    "    plt.cla()  \n",
    "    plt.close(fig)\n",
    "    trim_image_whitespace(mean_shap_filepath, bgcolor=(255, 255, 255), fuzz=10)\n",
    "\n",
    "    \n",
    "    # SHAP Summary Plot\n",
    "    fig_summary, ax_summary = plt.subplots()\n",
    "    shap.summary_plot(shap_test, show=False)\n",
    "    summary_shap_filepath = os.path.join(summary_shap_dir, f\"{glacierid}_shap_summary_plot.png\")\n",
    "    plt.savefig(summary_shap_filepath, dpi=300, format='png', metadata={'Creator': 'Matplotlib'})\n",
    "    plt.clf()  # Clear the figure\n",
    "    plt.cla()  # Clear the axes\n",
    "    plt.close(fig_summary)\n",
    "\n",
    "    # SHAP Relationshp plot (seasonal color version)\n",
    "    rename_dict = {\n",
    "        'Air Temperature': 'Air Temp. (°C)',\n",
    "        'Runoff': 'Runoff (m³/s)',\n",
    "        'Velocity': 'Velocity (m/yr)',\n",
    "        'Strain Rate': 'Strain R. (yr$^{-1}$)',\n",
    "        'Surface Elevation': 'Surface Ele. (m)',\n",
    "        'Bed Elevation': 'Bed Ele. (m)',\n",
    "        'Thickness': 'Thickness (m)',\n",
    "        'Slope': 'Bed Slope (m)',\n",
    "        'Melange': 'Mélange (°C)',\n",
    "        'Ocean Thermal Forcing': 'OTF (°C)'\n",
    "    }\n",
    "    X_test = X_test.rename(columns=rename_dict)\n",
    "    variables = rel_shap_df.columns\n",
    "    nrows, ncols = 2, 5\n",
    "    \n",
    "    subplot_width = 2.5  \n",
    "    subplot_height = 4.9   \n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(5.9,3), sharey='row')\n",
    "    day_of_year = rel_shap_df.index.dayofyear\n",
    "    day_of_year_normalized = day_of_year / 365.0\n",
    "    cmap = plt.cm.twilight\n",
    "    colors = cmap(day_of_year_normalized)\n",
    "    \n",
    "    for i, variable in enumerate(variables):\n",
    "        ax = axes[i // ncols, i % ncols]\n",
    "        shap_values = rel_shap_df[variable].values\n",
    "        feature_values = X_test[variable].values\n",
    "        scatter = ax.scatter(\n",
    "            feature_values,\n",
    "            shap_values,\n",
    "            c=day_of_year_normalized,\n",
    "            s=0.2,\n",
    "            cmap=cmap,\n",
    "        )\n",
    "        ax.set_title(variable, y=-.5) \n",
    "    \n",
    "        # Reduce tick label padding\n",
    "        ax.tick_params(axis='x', pad=2)\n",
    "        if abs(feature_values.max() - feature_values.min()) < 0.01:\n",
    "            ax.xaxis.set_major_locator(plt.MaxNLocator(nbins=3))\n",
    "            ax.ticklabel_format(axis='x', style='plain', useOffset=False)\n",
    "\n",
    "        # strain rate label customization\n",
    "        if variable == 'Strain R. (yr$^{-1}$)':\n",
    "            xticks = ax.get_xticks()\n",
    "            if np.any(np.abs(xticks) < 0.1) and np.any(np.abs(xticks) > 0):  \n",
    "                ax.ticklabel_format(axis='x', style='sci', scilimits=(-2, 2))  \n",
    "                for label in ax.get_xticklabels():\n",
    "                    label.set_verticalalignment('top')  \n",
    "                    label.set_y(0.25)\n",
    "    \n",
    "        if i % ncols == 0:\n",
    "            ax.set_ylabel(\"SHAP value\")\n",
    "            ax.set_yticks(range(-100, 101, 50))  # fewer ticks\n",
    "        else:\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_ylim(-100, 100)\n",
    "    \n",
    "    # Positioning the colorbar to the right of all plots\n",
    "    cbar_ax = fig.add_axes([0.88, 0.15, 0.02, 0.7]) \n",
    "    cbar = fig.colorbar(scatter, cax=cbar_ax)\n",
    "    season_locs = [5, 128, 255, 383, 510]\n",
    "    season_labels = ['Winter', 'Spring', 'Summer', 'Fall', 'Winter']\n",
    "    cbar.set_ticks(season_locs)\n",
    "    cbar.set_ticklabels(season_labels)\n",
    "    plt.subplots_adjust(left=0.1, right=0.85, top=0.85, bottom=0.15, wspace=0.3, hspace=0.5)\n",
    "    \n",
    "    relationships_plot_dir = \"/Users/kevin/Documents/ML_longterm/XGBoost/seasonal/full_features/outputs/shap_relationships_plot\"\n",
    "    os.makedirs(relationships_plot_dir, exist_ok=True)\n",
    "    relationships_plot_filepath = os.path.join(relationships_plot_dir, f\"{glacierid}_relationships_plot.png\")\n",
    "    plt.savefig(relationships_plot_filepath, dpi=300, bbox_inches='tight', format='png', metadata={'Creator': 'Matplotlib'})\n",
    "    plt.clf()  \n",
    "    plt.cla()  \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79daaf3d-3b12-46bd-8245-1e8d6fd0b004",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
