{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b81b180",
   "metadata": {},
   "source": [
    "## Create main dataframe (df) by glacier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c843272",
   "metadata": {},
   "source": [
    "Import data after interpolating and running an SSA. Clips data to a common date range. For an individual glacier, adds all used data to a single pandas dataframe (df) in preparation for use in an XGBoost or similar model.\n",
    "\n",
    "Uses \"qualifying_glacierids.json\", a list of glacierid's that we have full data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from matplotlib.pyplot import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import time\n",
    "from datetime import date\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ef58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths and glacier IDs\n",
    "filepath = \"/Users/.../variables_data/\"\n",
    "iddirectory = \"/Users/.../SSA/\"\n",
    "glacierids = []\n",
    "    \n",
    "# pre-found qualifying glacierids\n",
    "file_path = 'qualifying_glacierids.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    glacierids = json.load(file)\n",
    "\n",
    "# Initialize dictionaries to store DataFrames for each input variable\n",
    "dataframes = {\n",
    "    \"Terminus Position\": {},\n",
    "    \"Air Temperature\": {},\n",
    "    \"Discharge-MAR\": {},\n",
    "    \"Discharge-RACMO\": {},\n",
    "    \"Velocity\": {},\n",
    "    \"Strain Rate\": {},\n",
    "    \"Surface Elevation\": {},\n",
    "    \"Bed Elevation\": {},\n",
    "    \"Thickness\": {},\n",
    "    \"Bed Slope\": {},\n",
    "    \"Melange\": {},\n",
    "    \"OTF-EN4\": {},\n",
    "    \"OTF-ECCO\": {},\n",
    "}\n",
    "\n",
    "# Iterate through each glacier ID and read the corresponding data files\n",
    "for glacierid in glacierids:\n",
    "    try:\n",
    "        # Terminus position\n",
    "        dataframes[\"Terminus Position\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_terminus_position_combined_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Terminus Position\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Terminus Position\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Terminus Position\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Air temperature\n",
    "        dataframes[\"Air Temperature\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_air_temperature_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Air Temperature\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Air Temperature\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Air Temperature\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Discharge MAR\n",
    "        dataframes[\"Discharge-MAR\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_discharge_MAR_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Discharge-MAR\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Discharge - MAR\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Discharge-MAR\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Discharge RACMO\n",
    "        dataframes[\"Discharge-RACMO\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_discharge_RACMO_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Discharge-RACMO\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Discharge - RACMO\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Discharge-RACMO\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Velocity\n",
    "        dataframes[\"Velocity\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_velocity_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Velocity\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Velocity\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Velocity\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Strain rate\n",
    "        dataframes[\"Strain Rate\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_strain_rate_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Strain Rate\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Strain Rate\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Strain Rate\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Surface elevation\n",
    "        dataframes[\"Surface Elevation\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_surface_elevation_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Surface Elevation\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Surface Elevation\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Surface Elevation\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Bed elevation\n",
    "        dataframes[\"Bed Elevation\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_bed_elevation_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Bed Elevation\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Bed Elevation\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Bed Elevation\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Thickness\n",
    "        dataframes[\"Thickness\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_thickness_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Thickness\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Ice Thickness\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Thickness\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Bed slope\n",
    "        dataframes[\"Bed Slope\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_slope_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Bed Slope\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Bed Slope\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Bed Slope\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # Melange\n",
    "        dataframes[\"Melange\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_melange_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"Melange\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Melange\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"Melange\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # OTF EN4\n",
    "        dataframes[\"OTF-EN4\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_OTF_EN4_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"OTF-EN4\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Ocean Thermal Forcing - EN4\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"OTF-EN4\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "        # OTF ECCO\n",
    "        dataframes[\"OTF-ECCO\"][glacierid] = pd.read_csv(\n",
    "            f\"{filepath}.../{glacierid}_OTF_ECCO_SSA.csv\",\n",
    "            skiprows=1, header=None,\n",
    "        )\n",
    "        dataframes[\"OTF-ECCO\"][glacierid].columns = [\n",
    "            \"Decimal Date\",\n",
    "            \"Ocean Thermal Forcing - ECCO\",\n",
    "            \"Integer Date\",\n",
    "            \"Longterm Comp\",\n",
    "            \"Seasonal Comp\",\n",
    "        ]\n",
    "        \n",
    "        dataframes[\"OTF-ECCO\"][glacierid].drop(columns=[\"Decimal Date\"], inplace=True)\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found for glacier {glacierid}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing glacier {glacierid}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f8242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the variables and glacier IDs in the dataframes dictionary\n",
    "for variable, glacier_data in dataframes.items():\n",
    "    for glacierid, df in glacier_data.items():\n",
    "        try:\n",
    "            # Convert 'Integer Date' to datetime and create the 'Date' column\n",
    "            df['Date'] = pd.to_datetime(df['Integer Date'].astype(str).str.split('.').str[0], format='%Y%m%d')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing 'Date' column for {variable}, Glacier {glacierid}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check where seasonal comp is a 0-value\n",
    "for variable, glacier_data in dataframes.items():\n",
    "    for glacierid, df in glacier_data.items():\n",
    "        try:\n",
    "            # Ensure 'Seasonal Comp' column is dtype float64\n",
    "            df['Seasonal Comp'] = pd.to_numeric(df['Seasonal Comp'], errors='coerce')\n",
    "\n",
    "            # Check for blank or zero values in 'Seasonal Comp'\n",
    "            if df['Seasonal Comp'].isna().all() or (df['Seasonal Comp'] == 0).all():\n",
    "                # Print the name of the DataFrame\n",
    "                print(f\"Updating 'Seasonal Comp' for {variable}, Glacier {glacierid}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing 'Seasonal Comp' for {variable}, Glacier {glacierid}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccab279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range to check\n",
    "start_date = pd.Timestamp(\"2000-01-01\")\n",
    "end_date = pd.Timestamp(\"2020-05-01\")\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "# Iterate through the variables and glacier IDs in the dataframes dictionary\n",
    "for variable, glacier_data in dataframes.items():\n",
    "    for glacierid, df in glacier_data.items():\n",
    "        try:\n",
    "            # Ensure the 'Date' column exists\n",
    "            if 'Date' in df.columns:\n",
    "                # Find the missing dates\n",
    "                missing_dates = date_range.difference(df['Date'])\n",
    "                \n",
    "                if not missing_dates.empty:\n",
    "                    # Create a DataFrame with the missing dates\n",
    "                    missing_df = pd.DataFrame({'Date': missing_dates})\n",
    "                    \n",
    "                    # Add the missing rows to the original DataFrame\n",
    "                    df = pd.concat([df, missing_df], ignore_index=True)\n",
    "                    \n",
    "                    # Sort by 'Date' and reset the index\n",
    "                    df = df.sort_values(by=\"Date\").reset_index(drop=True)\n",
    "                \n",
    "                # Update the DataFrame in the dictionary\n",
    "                dataframes[variable][glacierid] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing missing dates for {variable}, Glacier {glacierid}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cf67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary for the combined data\n",
    "dataframes['Main'] = {}\n",
    "\n",
    "# Iterate through all glacier IDs\n",
    "for glacierid in glacierids:\n",
    "    try:\n",
    "        # Create a base DataFrame with the full date range\n",
    "        start_date = pd.Timestamp(\"2000-01-01\")\n",
    "        end_date = pd.Timestamp(\"2020-05-01\")\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "        combined_df = pd.DataFrame({'Date': date_range})\n",
    "        \n",
    "        # Iterate through variables to add 'Seasonal Comp' columns\n",
    "        for variable, glacier_data in dataframes.items():\n",
    "            if variable == \"Main\":  # Skip the Main key\n",
    "                continue\n",
    "            \n",
    "            # Check if the glacierid exists in the current variable\n",
    "            if glacierid in glacier_data:\n",
    "                # Extract the 'Seasonal Comp' column\n",
    "                seasonal_comp = glacier_data[glacierid][['Date', 'Seasonal Comp']].copy()\n",
    "                # Rename the column to include the variable name\n",
    "                seasonal_comp.rename(columns={'Seasonal Comp': f'Seasonal {variable}'}, inplace=True)\n",
    "                # Merge with the combined DataFrame\n",
    "                combined_df = pd.merge(combined_df, seasonal_comp, on='Date', how='left')\n",
    "            else:\n",
    "                # If the variable does not exist for the glacier, add a blank column\n",
    "                combined_df[f'Seasonal {variable}'] = None\n",
    "        \n",
    "        # Store the combined DataFrame in the 'Main' dictionary\n",
    "        dataframes['Main'][glacierid] = combined_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing glacier {glacierid}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bc9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the main dataframes\n",
    "output_directory = \".../all_features_per_glacier\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through each glacier ID in the 'Main' dictionary\n",
    "for glacierid, df in dataframes['Main'].items():\n",
    "    # Define the output file path\n",
    "    output_file = os.path.join(output_directory, f\"{glacierid}_main_df.csv\")\n",
    "    \n",
    "    # Save the dataframe as a .csv file\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1149443a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
